#!/usr/bin/env python3
"""
–ü—Ä–æ—Ç–æ—Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã –ª–∏–Ω–∫–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç—å—é
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º —Å–≤—è–∑—ã–≤–∞–Ω–∏—è
"""
import re
import os
import time
from typing import List, Dict, Any, Tuple
import difflib

class SemanticObjectLinker:
    """–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ç–æ—Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã –ª–∏–Ω–∫–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç—å—é"""
    
    def __init__(self):
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
        self.reference_patterns = {
            'Figure': re.compile(r'\b(?:Figure|Fig\.?|–†–∏—Å\.?|–†–∏—Å—É–Ω–æ–∫)\s*([0-9]+(?:\.[0-9]+)*[a-z]?)', re.IGNORECASE),
            'Table': re.compile(r'\b(?:Table|Tab\.?|–¢–∞–±–ª–∏—Ü–∞|–¢–∞–±–ª\.?)\s*([0-9]+(?:\.[0-9]+)*[a-z]?)', re.IGNORECASE),
            'Equation': re.compile(r'\b(?:Equation|Eq\.?|–£—Ä–∞–≤–Ω–µ–Ω–∏–µ|–£—Ä\.?)\s*\(?([0-9]+(?:\.[0-9]+)*)\)?', re.IGNORECASE),
            'Section': re.compile(r'\b(?:Section|Sec\.?|–†–∞–∑–¥–µ–ª|¬ß)\s*([0-9]+(?:\.[0-9]+)*)', re.IGNORECASE),
            'Lemma': re.compile(r'\b(?:Lemma|–õ–µ–º–º–∞)\s*([0-9]+(?:\.[0-9]+)*)', re.IGNORECASE),
            'Theorem': re.compile(r'\b(?:Theorem|–¢–µ–æ—Ä–µ–º–∞)\s*([0-9]+(?:\.[0-9]+)*)', re.IGNORECASE),
            'Proposition': re.compile(r'\b(?:Proposition|Prop\.?|–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ)\s*([0-9]+(?:\.[0-9]+)*)', re.IGNORECASE),
            'Corollary': re.compile(r'\b(?:Corollary|Cor\.?|–°–ª–µ–¥—Å—Ç–≤–∏–µ)\s*([0-9]+(?:\.[0-9]+)*)', re.IGNORECASE),
            'Formula': re.compile(r'\(([0-9]+)\)'),
            'Citation': re.compile(r'\[([0-9]+(?:,\s*[0-9]+)*)\]')
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
        self.object_patterns = {
            'Figure': re.compile(r'\b(?:Figure|Fig\.?|–†–∏—Å\.?|–†–∏—Å—É–Ω–æ–∫)\s*([0-9]+(?:\.[0-9]+)*[a-z]?)[:.]', re.IGNORECASE),
            'Table': re.compile(r'\b(?:Table|Tab\.?|–¢–∞–±–ª–∏—Ü–∞|–¢–∞–±–ª\.?)\s*([0-9]+(?:\.[0-9]+)*[a-z]?)[:.]', re.IGNORECASE),
            'Section': re.compile(r'^([0-9]+(?:\.[0-9]+)*)\s+[A-Z–ê-–Ø]', re.MULTILINE),
            'Proposition': re.compile(r'\b(?:Proposition|Prop\.?|–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ)\s*([0-9]+(?:\.[0-9]+)*)[:.]', re.IGNORECASE),
            'Corollary': re.compile(r'\b(?:Corollary|Cor\.?|–°–ª–µ–¥—Å—Ç–≤–∏–µ)\s*([0-9]+(?:\.[0-9]+)*)[:.]', re.IGNORECASE),
            'Lemma': re.compile(r'\b(?:Lemma|–õ–µ–º–º–∞)\s*([0-9]+(?:\.[0-9]+)*)[:.]', re.IGNORECASE),
            'Theorem': re.compile(r'\b(?:Theorem|–¢–µ–æ—Ä–µ–º–∞)\s*([0-9]+(?:\.[0-9]+)*)[:.]', re.IGNORECASE)
        }
        
        # –°–ª–æ–≤–∞—Ä–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
        self.semantic_synonyms = {
            'Figure': ['Figure', 'Fig', '–†–∏—Å', '–†–∏—Å—É–Ω–æ–∫', '–¥–∏–∞–≥—Ä–∞–º–º–∞', '–≥—Ä–∞—Ñ–∏–∫', '—Å—Ö–µ–º–∞', '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ'],
            'Table': ['Table', 'Tab', '–¢–∞–±–ª–∏—Ü–∞', '–¢–∞–±–ª', '—Å–≤–æ–¥–∫–∞', '–¥–∞–Ω–Ω—ã–µ'],
            'Equation': ['Equation', 'Eq', '–£—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–£—Ä', '—Ñ–æ—Ä–º—É–ª–∞', '–≤—ã—Ä–∞–∂–µ–Ω–∏–µ'],
            'Section': ['Section', 'Sec', '–†–∞–∑–¥–µ–ª', '–≥–ª–∞–≤–∞', '—á–∞—Å—Ç—å', '–ø–∞—Ä–∞–≥—Ä–∞—Ñ'],
            'Lemma': ['Lemma', '–õ–µ–º–º–∞', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ', '—Ñ–∞–∫—Ç'],
            'Theorem': ['Theorem', '–¢–µ–æ—Ä–µ–º–∞', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç'],
            'Proposition': ['Proposition', 'Prop', '–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ', '—Ç–µ–∑–∏—Å'],
            'Corollary': ['Corollary', 'Cor', '–°–ª–µ–¥—Å—Ç–≤–∏–µ', '–≤—ã–≤–æ–¥', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç'],
            'Formula': ['Formula', '–§–æ—Ä–º—É–ª–∞', '–≤—ã—Ä–∞–∂–µ–Ω–∏–µ', '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ'],
            'Citation': ['Citation', '–¶–∏—Ç–∞—Ç–∞', '—Å—Å—ã–ª–∫–∞', '–∏—Å—Ç–æ—á–Ω–∏–∫', '—Ä–∞–±–æ—Ç–∞']
        }
    
    def extract_text_from_ocr(self, file_path: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ OCR –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except:
            try:
                with open(file_path, 'r', encoding='latin-1') as f:
                    lines = f.readlines()
            except:
                return ""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ —Å –∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
        words_data = []
        for line in lines:
            parts = line.strip().split('\t')
            if len(parts) >= 10:
                word = parts[0]
                try:
                    y1, x1 = int(parts[2]), int(parts[1])
                    word_type = parts[-1]
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Ç–∏–ø—ã
                    if word_type in ['paragraph', 'abstract', 'title', 'section']:
                        words_data.append((y1, x1, word))
                except (ValueError, IndexError):
                    continue
        
        if not words_data:
            return ""
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
        words_data.sort(key=lambda x: (x[0], x[1]))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –≤ —Å—Ç—Ä–æ–∫–∏
        lines_text = []
        current_line = []
        current_y = None
        
        for y, x, word in words_data:
            if current_y is None or abs(y - current_y) <= 15:  # –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞
                current_line.append(word)
                current_y = y
            else:
                if current_line:
                    lines_text.append(' '.join(current_line))
                current_line = [word]
                current_y = y
        
        if current_line:
            lines_text.append(' '.join(current_line))
        
        return '\n'.join(lines_text)
    
    def find_references(self, text: str) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""
        references = []
        
        for ref_type, pattern in self.reference_patterns.items():
            for match in pattern.finditer(text):
                references.append({
                    'text': match.group(0),
                    'type': ref_type,
                    'id': match.group(1),
                    'start': match.start(),
                    'end': match.end(),
                    'linked_objects': [],
                    'context': self.extract_context(text, match.start(), match.end())
                })
        
        return references
    
    def extract_context(self, text: str, start: int, end: int, window: int = 50) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏"""
        context_start = max(0, start - window)
        context_end = min(len(text), end + window)
        return text[context_start:context_end].strip()
    
    def find_objects(self, text: str) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—ä–µ–∫—Ç—ã (–∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∏—Å—É–Ω–∫–æ–≤, —Ç–∞–±–ª–∏—Ü –∏ —Ç.–¥.)"""
        objects = []
        
        for obj_type, pattern in self.object_patterns.items():
            for match in pattern.finditer(text):
                obj_id = match.group(1) if match.groups() else ''
                objects.append({
                    'text': match.group(0),
                    'type': obj_type,
                    'id': obj_id,
                    'start': match.start(),
                    'end': match.end(),
                    'context': self.extract_context(text, match.start(), match.end(), 100)
                })
        
        return objects
    
    def calculate_id_similarity(self, ref_id: str, obj_id: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏"""
        if not ref_id or not obj_id:
            return 0.0
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if ref_id == obj_id:
            return 1.0
        
        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if ref_id in obj_id or obj_id in ref_id:
            return 0.8
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º difflib –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        similarity = difflib.SequenceMatcher(None, ref_id.lower(), obj_id.lower()).ratio()
        return similarity if similarity > 0.6 else 0.0
    
    def calculate_semantic_similarity(self, ref_type: str, obj_type: str, ref_context: str, obj_context: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É —Å—Å—ã–ª–∫–æ–π –∏ –æ–±—ä–µ–∫—Ç–æ–º"""
        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤
        if ref_type != obj_type:
            return 0.0
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        ref_words = set(ref_context.lower().split())
        obj_words = set(obj_context.lower().split())
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        common_words = ref_words.intersection(obj_words)
        if not ref_words or not obj_words:
            return 0.5  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        word_similarity = len(common_words) / max(len(ref_words), len(obj_words))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        synonyms = self.semantic_synonyms.get(ref_type, [])
        synonym_score = 0.0
        
        for synonym in synonyms:
            if synonym.lower() in ref_context.lower() and synonym.lower() in obj_context.lower():
                synonym_score += 0.1
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ (–æ–±—ä–µ–∫—Ç—ã –æ–±—ã—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –¥–æ —Å—Å—ã–ª–æ–∫)
        positional_bonus = 0.1  # –Ω–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        total_similarity = word_similarity + min(synonym_score, 0.3) + positional_bonus
        return min(total_similarity, 1.0)
    
    def calculate_combined_similarity(self, ref: Dict, obj: Dict) -> float:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (ID + —Å–µ–º–∞–Ω—Ç–∏–∫–∞)"""
        id_sim = self.calculate_id_similarity(ref['id'], obj['id'])
        semantic_sim = self.calculate_semantic_similarity(
            ref['type'], obj['type'], 
            ref['context'], obj['context']
        )
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è (ID –≤–∞–∂–Ω–µ–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏)
        combined = 0.7 * id_sim + 0.3 * semantic_sim
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if ref['type'] == obj['type'] and len(ref['context']) > 10 and len(obj['context']) > 10:
            combined += 0.1
        
        return min(combined, 1.0)
    
    def link_references_to_objects(self, references: List[Dict], objects: List[Dict]) -> None:
        """–°–≤—è–∑—ã–≤–∞–µ—Ç —Å—Å—ã–ª–∫–∏ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º"""
        for ref in references:
            ref_type = ref['type']
            
            # –ò—â–µ–º –≤—Å–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –æ–±—ä–µ–∫—Ç—ã
            candidates = []
            
            for obj in objects:
                if obj['type'] == ref_type:
                    similarity = self.calculate_combined_similarity(ref, obj)
                    
                    if similarity > 0.3:  # –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —É—á–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
                        candidates.append({
                            'object': obj,
                            'similarity': similarity,
                            'id_similarity': self.calculate_id_similarity(ref['id'], obj['id']),
                            'semantic_similarity': self.calculate_semantic_similarity(
                                ref['type'], obj['type'], 
                                ref['context'], obj['context']
                            )
                        })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –æ–±—â–µ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            candidates.sort(key=lambda x: x['similarity'], reverse=True)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            for candidate in candidates[:3]:  # —Ç–æ–ø-3 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                if candidate['similarity'] > 0.5:  # –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞
                    ref['linked_objects'].append({
                        'text': candidate['object']['text'],
                        'type': candidate['object']['type'],
                        'id': candidate['object']['id'],
                        'similarity': candidate['similarity'],
                        'id_similarity': candidate['id_similarity'],
                        'semantic_similarity': candidate['semantic_similarity'],
                        'context': candidate['object']['context']
                    })
    
    def generate_html_report(self, text: str, references: List[Dict], objects: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –æ—Ç—á–µ—Ç —Å –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        html_text = text
        offset = 0
        
        # –í—ã–¥–µ–ª—è–µ–º —Å—Å—ã–ª–∫–∏
        sorted_refs = sorted(references, key=lambda x: x['start'])
        
        for ref in sorted_refs:
            start = ref['start'] + offset
            end = ref['end'] + offset
            ref_text = ref['text']
            
            if ref['linked_objects']:
                css_class = 'linked'
                best_match = ref['linked_objects'][0]
                tooltip = f"–°–≤—è–∑–∞–Ω–æ —Å: {best_match['text']}<br/>–û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {best_match['similarity']:.2f}<br/>ID —Å—Ö–æ–∂–µ—Å—Ç—å: {best_match['id_similarity']:.2f}<br/>–°–µ–º–∞–Ω—Ç–∏–∫–∞: {best_match['semantic_similarity']:.2f}"
            else:
                css_class = 'unlinked'
                tooltip = "–û–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
            
            html_link = f'<span class="reference {css_class}" title="{tooltip}">{ref_text}</span>'
            html_text = html_text[:start] + html_link + html_text[end:]
            offset += len(html_link) - len(ref_text)
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π HTML –¥–æ–∫—É–º–µ–Ω—Ç
        return f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤ –Ω–∞—É—á–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }}
        .header {{
            background: linear-gradient(135deg, #007bff, #0056b3);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .stats {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }}
        .stat-item {{
            text-align: center;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 6px;
        }}
        .content {{
            background: white;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            white-space: pre-wrap;
            font-size: 14px;
            line-height: 1.8;
        }}
        .reference {{
            padding: 3px 8px;
            border-radius: 5px;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s;
            position: relative;
        }}
        .reference.linked {{
            background: linear-gradient(135deg, #d4edda, #c3e6cb);
            color: #155724;
            border: 1px solid #c3e6cb;
        }}
        .reference.unlinked {{
            background: linear-gradient(135deg, #f8d7da, #f5c6cb);
            color: #721c24;
            border: 1px solid #f5c6cb;
        }}
        .reference:hover {{
            transform: scale(1.08);
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
            z-index: 1000;
        }}
        .legend {{
            margin-top: 20px;
            padding: 20px;
            background: #e9ecef;
            border-radius: 8px;
        }}
        .legend-item {{
            display: inline-block;
            margin-right: 25px;
            margin-bottom: 8px;
        }}
        .semantic-info {{
            margin-top: 20px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .match-details {{
            background: #f8f9fa;
            padding: 15px;
            margin: 10px 0;
            border-radius: 6px;
            border-left: 4px solid #007bff;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫</h1>
        <p>–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ª–∏–Ω–∫–æ–≤–∞–Ω–∏—è —Å –ò–ò-–∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏</p>
    </div>
    
    <div class="stats">
        <div class="stat-item">
            <h4>üìÑ –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞</h4>
            <strong>{len(text)} —Å–∏–º–≤–æ–ª–æ–≤</strong>
        </div>
        <div class="stat-item">
            <h4>üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫</h4>
            <strong>{len(references)}</strong>
        </div>
        <div class="stat-item">
            <h4>‚úÖ –°–≤—è–∑–∞–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤</h4>
            <strong>{len([r for r in references if r['linked_objects']])}</strong>
        </div>
        <div class="stat-item">
            <h4>üìö –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤</h4>
            <strong>{len(objects)}</strong>
        </div>
        <div class="stat-item">
            <h4>üéØ –ü—Ä–æ—Ü–µ–Ω—Ç —Å–≤—è–∑—ã–≤–∞–Ω–∏—è</h4>
            <strong>{len([r for r in references if r['linked_objects']]) / max(len(references), 1) * 100:.1f}%</strong>
        </div>
        <div class="stat-item">
            <h4>üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑</h4>
            <strong>–í–∫–ª—é—á–µ–Ω</strong>
        </div>
    </div>
    
    <div class="content">{html_text}</div>
    
    <div class="semantic-info">
        <h3>üîç –î–µ—Ç–∞–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:</h3>
        {self.generate_semantic_details(references)}
    </div>
    
    <div class="legend">
        <h3>üé® –õ–µ–≥–µ–Ω–¥–∞:</h3>
        <div class="legend-item">
            <span class="reference linked">–°–≤—è–∑–∞–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞</span> - –Ω–∞–π–¥–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –æ–±—ä–µ–∫—Ç —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
        </div>
        <div class="legend-item">
            <span class="reference unlinked">–ù–µ—Å–≤—è–∑–∞–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞</span> - –æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–∏–∑–∫–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        </div>
    </div>
</body>
</html>
        """
    
    def generate_semantic_details(self, references: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ"""
        details = ""
        
        linked_refs = [r for r in references if r['linked_objects']]
        
        if not linked_refs:
            return "<p>–°–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.</p>"
        
        for ref in linked_refs[:5]:  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
            details += f"""
            <div class="match-details">
                <h4>üîó "{ref['text']}" (—Ç–∏–ø: {ref['type']})</h4>
                <p><strong>–ö–æ–Ω—Ç–µ–∫—Å—Ç:</strong> ...{ref['context']}...</p>
                <p><strong>–õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ:</strong> "{ref['linked_objects'][0]['text']}"</p>
                <p><strong>–ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏:</strong></p>
                <ul>
                    <li>–û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: <strong>{ref['linked_objects'][0]['similarity']:.3f}</strong></li>
                    <li>ID —Å—Ö–æ–∂–µ—Å—Ç—å: <strong>{ref['linked_objects'][0]['id_similarity']:.3f}</strong></li>
                    <li>–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: <strong>{ref['linked_objects'][0]['semantic_similarity']:.3f}</strong></li>
                </ul>
            </div>
            """
        
        return details
    
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ª–∏–Ω–∫–æ–≤–∞–Ω–∏–µ–º"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = self.extract_text_from_ocr(file_path)
        
        if not text:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç'}
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –∏ –æ–±—ä–µ–∫—Ç—ã
        references = self.find_references(text)
        objects = self.find_objects(text)
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        self.link_references_to_objects(references, objects)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º HTML –æ—Ç—á–µ—Ç
        html_content = self.generate_html_report(text, references, objects)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        linked_count = len([r for r in references if r['linked_objects']])
        total_refs = len(references)
        
        return {
            'text': text,
            'references': references,
            'objects': objects,
            'html_content': html_content,
            'statistics': {
                'text_length': len(text),
                'total_references': total_refs,
                'linked_references': linked_count,
                'total_objects': len(objects),
                'link_success_rate': linked_count / total_refs if total_refs > 0 else 0,
                'semantic_analysis_enabled': True,
                'average_similarity': sum(r['linked_objects'][0]['similarity'] for r in references if r['linked_objects']) / max(linked_count, 1)
            }
        }

def get_test_files() -> List[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    test_files = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π
    possible_paths = [
        "linking/ann",
        "linking/ann", 
        "ann"
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            for file in os.listdir(path):
                if file.endswith('.txt'):
                    test_files.append(os.path.join(path, file))
            break
    
    return test_files  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏

def run_semantic_demo():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    print("üß† –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ô –°–ò–°–¢–ï–ú–´ –õ–ò–ù–ö–û–í–ê–ù–ò–Ø –û–ë–™–ï–ö–¢–û–í")
    print("=" * 70)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
    linker = SemanticObjectLinker()
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
    os.makedirs("semantic_reports", exist_ok=True)
    
    # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_files = get_test_files()
    
    if not test_files:
        print("‚ùå –§–∞–π–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –ø–∞–ø–∫–∞—Ö: linking/ann/, linking/ann/, ann/")
        return
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(test_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    total_stats = {
        'total_files': 0,
        'total_references': 0,
        'total_linked': 0,
        'total_processing_time': 0
    }
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for i, file_path in enumerate(test_files, 1):
        print(f"\n{'='*70}")
        print(f"–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó {i}/{len(test_files)}: {os.path.basename(file_path)}")
        print(f"{'='*70}")
        
        try:
            start_time = time.time()
            results = linker.analyze_document(file_path)
            processing_time = time.time() - start_time
            
            if 'error' in results:
                print(f"‚ùå –û—à–∏–±–∫–∞: {results['error']}")
                continue
            
            stats = results['statistics']
            total_stats['total_files'] += 1
            total_stats['total_references'] += stats['total_references']
            total_stats['total_linked'] += stats['linked_references']
            total_stats['total_processing_time'] += processing_time
            
            print(f"‚ö° –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time*1000:.1f}–º—Å")
            print(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {stats['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {stats['total_references']}")
            print(f"üìö –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {stats['total_objects']}")
            print(f"‚úÖ –°–≤—è–∑–∞–Ω–æ: {stats['linked_references']}")
            print(f"üéØ –ü—Ä–æ—Ü–µ–Ω—Ç —Å–≤—è–∑—ã–≤–∞–Ω–∏—è: {stats['link_success_rate']*100:.1f}%")
            print(f"üß† –°—Ä–µ–¥–Ω—è—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {stats['average_similarity']:.3f}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            if results['references']:
                print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:")
                for ref in results['references'][:3]:
                    if ref['linked_objects']:
                        best = ref['linked_objects'][0]
                        print(f"  ‚úÖ '{ref['text']}' ‚Üí '{best['text']}'")
                        print(f"     üìä –û–±—â–∞—è: {best['similarity']:.3f}, ID: {best['id_similarity']:.3f}, –°–µ–º–∞–Ω—Ç–∏–∫–∞: {best['semantic_similarity']:.3f}")
                    else:
                        print(f"  ‚ùå '{ref['text']}' - –æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –æ—Ç—á–µ—Ç
            html_filename = f"semantic_reports/semantic_analysis_{i}.html"
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(results['html_content'])
            
            print(f"\nüìÑ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_filename}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if total_stats['total_files'] > 0:
        print(f"\n{'='*70}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
        print(f"{'='*70}")
        print(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_stats['total_files']}")
        print(f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {total_stats['total_references']}")
        print(f"‚úÖ –°–≤—è–∑–∞–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {total_stats['total_linked']}")
        print(f"üéØ –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–≤—è–∑—ã–≤–∞–Ω–∏—è: {total_stats['total_linked']/max(total_stats['total_references'], 1)*100:.1f}%")
        print(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_stats['total_processing_time']/total_stats['total_files']*1000:.1f}–º—Å")
        print(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –í–ö–õ–Æ–ß–ï–ù")
    
    print(f"\nüéâ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É semantic_reports/ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

if __name__ == "__main__":
    run_semantic_demo() 